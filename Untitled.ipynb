{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import gensim\n",
    "import spacy\n",
    "import textacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two sentences that are known to be paraphrases, pick the phrases that are similar and\n",
    "# contribute to their overall similarity:\n",
    "# Example 1:\n",
    "# Charlie Chan is off the case for the Fox Movie Channel.\n",
    "# The Fox Movie Channel has banned Charlie Chan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Charlie Chan is off the case for the Fox Movie Channel.\"\n",
    "# sent1 = \"PCCW's chief operating officer, Mike Butcher, and Alex Arena, the chief financial officer, will report directly to Mr So.\"\n",
    "sent2 = \"The Fox Movie Channel has banned Charlie Chan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the phrases from the sentences\n",
    "def extract_phrases(doc):\n",
    "    phrase_list = []\n",
    "    # Extracting the noun phrase from the sentence\n",
    "    for chunk in doc.noun_chunks:\n",
    "         phrase_list.append(str(chunk).lower())\n",
    "      \n",
    "    #Extracting the verb phrase from the sentence\n",
    "    pattern =  r'<VERB>?<ADV>*<VERB>+'\n",
    "    lists = textacy.extract.pos_regex_matches(doc, pattern)\n",
    "    for list in lists:\n",
    "         phrase_list.append(list.text.lower())\n",
    "    \n",
    "    return phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the sentence\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9\\s]','', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "                      \n",
    "# For Removing the stopwords\n",
    "# # Removing stopwords\n",
    "# def Remove_stopwords(sentence):\n",
    "#     return ' '.join([token for token in sentence.split() if token not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlie Chan is off the case for the Fox Movie Channel.\n",
      "Charlie Chan is off the case for the Fox Movie Channel\n"
     ]
    }
   ],
   "source": [
    "sent1 = clean_sentence(sent1)\n",
    "sent2 = clean_sentence(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n"
     ]
    }
   ],
   "source": [
    "sent1_phr_lst = extract_phrases(nlp(sent1))\n",
    "sent2_phr_lst = extract_phrases(nlp(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charlie chan', 'the case', 'the fox movie channel', 'is']\n",
      "['the fox movie channel', 'charlie chan', 'has banned']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_phr_lst)\n",
    "print(sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 35)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training word2vec model.\n",
    "sentences = [sent1_phr_lst,sent2_phr_lst]\n",
    "model = Word2Vec(min_count=1)\n",
    "model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charlie chan , charlie chan , 1.0\n",
      "the fox movie channel , the fox movie channel , 1.0\n"
     ]
    }
   ],
   "source": [
    "# Finding the phrases with the highest similarity\n",
    "for ph1 in sent1_phr_lst:\n",
    "    max_sim = 0\n",
    "    ph_sim =''\n",
    "    for ph2 in sent2_phr_lst:\n",
    "        if max_sim == 0:\n",
    "            # Cosine similarity between two words.\n",
    "            max_sim = model.similarity(ph1,ph2)\n",
    "            ph_sim = ph2\n",
    "        elif max_sim <= model.similarity(ph1,ph2):\n",
    "            max_sim = model.similarity(ph1,ph2)\n",
    "            ph_sim = ph2\n",
    "            \n",
    "    if max_sim >= 0:\n",
    "        print(ph1,',',ph_sim,',',max_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018422894"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading word2vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/ankit/Desktop/Irfan/NLMatics/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin',binary=True,limit=500000) \n",
    "model.similarity('is', 'banned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "model = FastText(size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(sentences=common_texts)\n",
    "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70550853"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('human' ,  'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"off\", None, nlp(\"is off\"))\n",
    "doc = nlp(\"The Fox Movie Channel has banned Charlie Chan.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "assert \"OBAMA\" not in matcher\n",
    "matcher.add(\"OBAMA\", None, nlp(\"Barack Obama\"))\n",
    "assert \"OBAMA\" in matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\textacy\\extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import spacy,en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"Barack Obama lifts America one last time in emotional farewell\"\n",
    "# pattern = r'<VERB>?<ADV>*<VERB>+'\n",
    "pattern =  r'<PREP> <DET>? (<NOUN>+<ADP>)* <NOUN>+'\n",
    "doc = nlp(sentence)\n",
    "lists = textacy.extract.pos_regex_matches(doc, pattern)\n",
    "for list in lists:\n",
    "    print(list.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# find approriate sense of word\n",
    "deer = wn.synset('deer.n.01')\n",
    "elk =wn.synset('elk.n.01')\n",
    "\n",
    "# find path similarity\n",
    "deer.path_similarity(elk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases<19 vocab, min_count=2, threshold=7, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "# Extracting phrases using gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=2,\n",
    "                      threshold=7)\n",
    "    print(phrases)\n",
    "    return Phraser(phrases)\n",
    "\n",
    "phrase= build_phrases(\"Barack Obama lifts America one last time in emotional farewell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fox_Movie', 'Channel', 'has', 'banned', 'Charlie', 'Chan']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "# documents = [\"the mayor of new york was there\", \"machine learning can be useful sometimes\",\"new york mayor was present\"]\n",
    "documents = [sent1 , sent2]\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "bigram = Phrases(sentence_stream, min_count=1, threshold=2)\n",
    "sent = ['The', 'Fox' ,'Movie', 'Channel', 'has', 'banned', 'Charlie', 'Chan']\n",
    "print(bigram[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 25)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['i','reports','him'], ['he','reported']]\n",
    "model = Word2Vec(min_count=1)\n",
    "model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05383833"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie Chan', 'the case', 'the Fox Movie Channel']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1_noun_phr_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Fox Movie Channel', 'Charlie Chan']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2_noun_phr_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
