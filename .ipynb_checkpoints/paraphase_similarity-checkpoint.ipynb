{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow_hub as hub\n",
    "import gensim\n",
    "import spacy\n",
    "import textacy\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can choose either of the two once embedding\n",
    "<ul>\n",
    "<li>Loading the googlenews word2vec\n",
    "<li>Create word embedding using MSR Corposes(or Any corposes which you intend to use)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading googlenews word2vec with a limit of 5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin',binary=True,limit=5000000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word embedding using the MSR Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split(' ')\n",
    "\n",
    "# Reading the MSR corpus \n",
    "df = pd.read_csv('msr_paraphrase_data.txt', sep='\\t', error_bad_lines=False)\n",
    "\n",
    "# Creating a list of the tokenized sentences\n",
    "df['String'] = df['String'].apply(tokenize)\n",
    "sentences = df['String'].to_list()\n",
    "\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the sentences\n",
    "if you wish you can go ahead and clean the sentences and remove the stopword using the below para however in my case I have not done that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the sentence\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9\\s]','', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "                      \n",
    "# Removing stopwords\n",
    "def Remove_stopwords(sentence):\n",
    "    return ' '.join([token for token in sentence.split() if token not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting prepositinal phrase\n",
    "def get_pps(doc):\n",
    "    \"Function to get PPs from a parsed document.\"\n",
    "    pps = []\n",
    "    for token in doc:\n",
    "        # Try this with other parts of speech for different subtrees.\n",
    "        if token.pos_ == 'ADP':\n",
    "            pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            pps.append(pp)\n",
    "    return pps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the phrases from the sentences here I have used space to extract the verb, noun ,prepositional phrase \n",
    "def extract_phrases(doc):\n",
    "    phrase_list = []\n",
    "    # Extracting the noun phrase from the sentence\n",
    "    for chunk in doc.noun_chunks:\n",
    "         phrase_list.append(str(chunk).lower())\n",
    "      \n",
    "    #Extracting the verb phrase from the sentence\n",
    "    pattern =  r'<VERB>?<ADV>*<VERB>+'\n",
    "    lists = textacy.extract.pos_regex_matches(doc, pattern)\n",
    "    for list in lists:\n",
    "         phrase_list.append(list.text.lower())\n",
    "            \n",
    "    # Extracting prepositinal phrase\n",
    "    phrase_list.extend(get_pps(doc))\n",
    "    \n",
    "    return phrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging the vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging the vector for each word in a phrase\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similarity\n",
    "Finding the phase with the highest similary however I have kept a minimum threshold of i.e the similarity > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the phrases with the highest similarity\n",
    "def similarity(phrase1, phrase2):  \n",
    "    for ph1 in phrase1:\n",
    "        max_sim = 0\n",
    "        ph_sim =''\n",
    "        for ph2 in phrase2:\n",
    "            if ph1 != ph2:\n",
    "                try:                    \n",
    "                    if max_sim == 0:\n",
    "                        # Cosine similarity between two words.\n",
    "                        max_sim = model.wv.n_similarity(ph1.lower().split(), ph2.lower().split())\n",
    "                        ph_sim = ph2\n",
    "                    elif max_sim <= model.wv.n_similarity(ph1.lower().split(), ph2.lower().split()):\n",
    "                        max_sim = model.wv.n_similarity(ph1.lower().split(), ph2.lower().split())\n",
    "                        ph_sim = ph2\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break\n",
    "\n",
    "        if max_sim >=0 and ph_sim and max_sim:\n",
    "            print(ph1,',',ph_sim,',',max_sim)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide the input sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Feelings about current business conditions improved substantially from the first quarter, jumping from 40 to 55.\"\n",
    "sent2 = \"Assessment of current business conditions improved substantially, the Conference Board said, jumping to 55 from 40 in the first quarter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = clean_sentence(sent1)\n",
    "sent1 = Remove_stopwords(sent1)\n",
    "\n",
    "sent2 = clean_sentence(sent2)\n",
    "sent2 = Remove_stopwords(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/anaconda3/lib/python3.7/site-packages/textacy/extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n",
      "/home/irfan/anaconda3/lib/python3.7/site-packages/textacy/extract.py:327: DeprecationWarning: `pos_regex_matches()` has been deprecated! for similar but more powerful and performant functionality, use `textacy.extract.matches()` instead.\n",
      "  action=\"once\",\n"
     ]
    }
   ],
   "source": [
    "sent1_phr_lst = extract_phrases(nlp(sent1))\n",
    "sent2_phr_lst = extract_phrases(nlp(sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output using by creating word embedding using MSR corpus\n",
    "No cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feelings', 'current business conditions', 'the first quarter', 'improved', 'jumping', 'about current business conditions', 'from the first quarter', 'from 40 to 55']\n",
      "['assessment', 'current business conditions', 'the conference board', 'the first quarter', 'improved', 'said', 'jumping', 'of current business conditions', 'to 55', 'from 40 in the first quarter', 'in the first quarter']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_phr_lst)\n",
    "print(sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feelings , assessment , 0.9543698\n",
      "current business conditions , improved , 0.99947375\n",
      "the first quarter , the conference board , 0.9999101\n",
      "improved , current business conditions , 0.99947375\n",
      "jumping , the conference board , 0.9972994\n",
      "about current business conditions , current business conditions , 0.99991834\n",
      "from the first quarter , from 40 in the first quarter , 0.9999393\n",
      "from 40 to 55 , to 55 , 0.99947983\n"
     ]
    }
   ],
   "source": [
    "similarity(sent1_phr_lst, sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feelings current business conditions', 'improved', 'jumping']\n",
      "['assessment current business conditions', 'conference board', '55 40 quarter', 'improved', 'said jumping']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_phr_lst)\n",
    "print(sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feelings current business conditions , assessment current business conditions , 0.9999904\n",
      "improved , assessment current business conditions , 0.9994738\n",
      "jumping , assessment current business conditions , 0.99721813\n"
     ]
    }
   ],
   "source": [
    "similarity(sent1_phr_lst, sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output using Word2vec google new\n",
    "No cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feelings', 'current business conditions', 'the first quarter', 'improved', 'jumping', 'about current business conditions', 'from the first quarter', 'from 40 to 55']\n",
      "['assessment', 'current business conditions', 'the conference board', 'the first quarter', 'improved', 'said', 'jumping', 'of current business conditions', 'to 55', 'from 40 in the first quarter', 'in the first quarter']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_phr_lst)\n",
    "print(sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'of' not in vocabulary\"\n",
      "feelings , current business conditions , 0.1891333\n",
      "\"word 'of' not in vocabulary\"\n",
      "current business conditions , the first quarter , 0.30059728\n",
      "\"word 'of' not in vocabulary\"\n",
      "the first quarter , the conference board , 0.35872227\n",
      "\"word 'of' not in vocabulary\"\n",
      "improved , the first quarter , 0.27405083\n",
      "\"word 'of' not in vocabulary\"\n",
      "jumping , the first quarter , 0.1505425\n",
      "\"word 'of' not in vocabulary\"\n",
      "about current business conditions , current business conditions , 0.92669064\n",
      "\"word 'of' not in vocabulary\"\n",
      "from the first quarter , the first quarter , 0.9443298\n",
      "\"word '40' not in vocabulary\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/irfan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "/home/irfan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "similarity(sent1_phr_lst, sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feelings current business conditions', 'improved', 'jumping']\n",
      "['assessment current business conditions', 'conference board', '55 40 quarter', 'improved', 'said jumping']\n"
     ]
    }
   ],
   "source": [
    "print(sent1_phr_lst)\n",
    "print(sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word '55' not in vocabulary\"\n",
      "feelings current business conditions , assessment current business conditions , 0.7705159\n",
      "\"word '55' not in vocabulary\"\n",
      "improved , assessment current business conditions , 0.26115558\n",
      "\"word '55' not in vocabulary\"\n",
      "jumping , conference board , 0.082201235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irfan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/irfan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n",
      "/home/irfan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "similarity(sent1_phr_lst, sent2_phr_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
